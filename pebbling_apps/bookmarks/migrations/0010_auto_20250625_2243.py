# Generated by Django 5.1.6 on 2025-06-25 22:43

from django.db import migrations


def normalize_bookmark_hashes_forward(apps, schema_editor):
    """
    Update all existing bookmark hashes to use the new URL normalization algorithm.
    Handle duplicates by keeping the newer bookmark and deleting older ones.
    """
    import logging
    import json
    from django.db import transaction, IntegrityError
    from datetime import datetime

    # Get the historical model
    Bookmark = apps.get_model("bookmarks", "Bookmark")

    # Import the actual URLNormalizer service to avoid code duplication
    from pebbling_apps.bookmarks.services import URLNormalizer

    # Set up logging
    logger = logging.getLogger("bookmarks.migration")

    # Initialize normalizer
    normalizer = URLNormalizer()

    # Get total count for progress tracking
    total_bookmarks = Bookmark.objects.count()
    logger.info(f"Starting hash normalization for {total_bookmarks} bookmarks")

    # Initialize counters
    processed = 0
    duplicates_found = 0
    bookmarks_deleted = 0

    # Process bookmarks one by one
    for bookmark in Bookmark.objects.iterator():
        try:
            with transaction.atomic():
                # Calculate new hash
                new_hash = normalizer.generate_hash(bookmark.url)

                # Try to update the bookmark with the new hash
                bookmark.unique_hash = new_hash
                bookmark.save(update_fields=["unique_hash"])

                processed += 1

                if processed % 100 == 0:
                    logger.info(f"Processed {processed}/{total_bookmarks} bookmarks")

        except IntegrityError:
            # Duplicate hash found - handle collision
            duplicates_found += 1

            try:
                with transaction.atomic():
                    # Find the existing bookmark with this hash
                    existing = Bookmark.objects.filter(
                        unique_hash=new_hash, owner=bookmark.owner
                    ).first()

                    if existing:
                        # Compare timestamps - keep the newer one
                        if bookmark.created_at > existing.created_at:
                            # Current bookmark is newer, delete the existing one
                            deleted_bookmark = existing
                            keep_bookmark = bookmark
                        else:
                            # Existing bookmark is newer, delete the current one
                            deleted_bookmark = bookmark
                            keep_bookmark = existing

                        # Log the deletion
                        deletion_log = {
                            "action": "deleted_duplicate",
                            "deleted_bookmark": {
                                "id": deleted_bookmark.id,
                                "url": deleted_bookmark.url,
                                "title": deleted_bookmark.title,
                                "owner_id": deleted_bookmark.owner_id,
                                "created_at": deleted_bookmark.created_at.isoformat(),
                                "updated_at": deleted_bookmark.updated_at.isoformat(),
                                "unique_hash": deleted_bookmark.unique_hash,
                                "tags": list(
                                    deleted_bookmark.tags.values_list("name", flat=True)
                                ),
                            },
                            "kept_bookmark_id": keep_bookmark.id,
                        }

                        logger.info(f"Duplicate found: {json.dumps(deletion_log)}")

                        # Delete the older bookmark
                        deleted_bookmark.delete()
                        bookmarks_deleted += 1

                        # If we kept the current bookmark, update its hash
                        if keep_bookmark.id == bookmark.id:
                            bookmark.unique_hash = new_hash
                            bookmark.save(update_fields=["unique_hash"])

                        processed += 1

            except Exception as e:
                logger.error(
                    f"Error handling duplicate for bookmark {bookmark.id}: {e}"
                )
                processed += 1

        except Exception as e:
            logger.error(f"Error processing bookmark {bookmark.id}: {e}")
            processed += 1

    # Log final statistics
    logger.info(f"Migration completed:")
    logger.info(f"  Total bookmarks processed: {processed}")
    logger.info(f"  Duplicates found: {duplicates_found}")
    logger.info(f"  Bookmarks deleted: {bookmarks_deleted}")
    logger.info(f"  Final bookmark count: {Bookmark.objects.count()}")


def normalize_bookmark_hashes_reverse(apps, schema_editor):
    """
    Reverse migration - no-op since we can't reverse hash normalization
    without losing the ability to detect previously normalized duplicates.
    """
    pass


class Migration(migrations.Migration):

    dependencies = [
        ("bookmarks", "0009_alter_bookmark_url"),
    ]

    # Set atomic=False to allow individual transactions for each bookmark
    atomic = False

    operations = [
        migrations.RunPython(
            normalize_bookmark_hashes_forward,
            normalize_bookmark_hashes_reverse,
        ),
    ]
